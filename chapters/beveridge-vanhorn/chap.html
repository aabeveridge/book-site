<p>Introduction</p>
<p>In her 2017 <em>Kairos</em> <a href="http://kairos.technorhetoric.net/21.2/topoi/gries/index.html"><span class="underline">article</span></a> “Mapping Obama Hope,” Laurie Gries heavily relies on data visualization to deepen our understanding of Obama Hope’s highly circulatory and distributed rhetorical life. As a data driven method, data visualization can be understood as the act of collecting data, processing it, and making sense of it visually through statistical methods. As a rhetorical product, a data visualization can be as simple as an embedded static image that compares categorical data—like a pie chart or a bar graph—or a dynamic digital interface, allowing people to interact with the data and select how they are compared, the type of graph or chart that will be used, and the time/date range that will frame the visual description. From a design standpoint, a data visualization can be as basic as creating a graph or chart in Microsoft Excel or Google Sheets, using hand coded data from an observational study, or a data visualization can be as complex as calculating network-wide trends for an entire social network—in real time—to display lists of the hottest topics and #hashtags for various regions/locations/networks. As pervasive and ubiquitous digital objects, data visualizations are embedded in everything from websites and news blogs to mobile applications and virtual dashboards to also, as evident in <em>Kairos</em> (see also Gallagher and DeVoss), online scholarly journals. Yet, given the vast diversity of types and possibilities for designing data visualizations, one thing remains true for all of them: data visualizations are only as effective as their underlying data and the methods/methodologies used to produce them.</p>
<p>This point may seem obvious or trite, but as this chapter argues, it is a mistake to assume that the pervasive ubiquity of data visuals are free from the entangled problems of data-driven content that now dominates our professional and personal lives. Data has surely become “the new oil”—a popular metaphor demonstrating the powerful capabilities of data driven methods—but the oil metaphor also forces us to consider the harmful pollutants these methods have introduced to digital networks and the negative consequences they have on academic research. In this chapter, we are particularly concerned with how such pollutants constrain the access digital visual studies scholars have to the data they need to generate macroscopic studies of visual artifacts such as Obama Hope. Because of their shifting status as a resource of value—both economically and culturally—data visualizations maintain an inverse relationship with the underlying data that informs their making. In other words, as data visualizations have become more wide-spread and easier to produce, their underlying data have become financially lucrative and, therefore, less accessible. This parodox generates significant challenges for digital visual studies scholars who want to use digital tools, replicate data-driven methods, peer review data-driven work, and access digital data to study visual artifacts at scale.</p>
<p>In the first section of this chapter, we explore some of these challenges not only so that digital visual studies scholars can remain cognizant of them as they move forward in this exciting area of research but also to identify the specific problem they pose for relying on Twitter data to do digital visual studies, which our own research presented here aims to address. But to be clear, we <em>do not</em> see these challenges as an essentialist disqualification of data-driven methods for digital visual research. In fact, quite the opposite. Much like the classical formulation of rhetoric, the tools and methods emerging in an era of data-driven content function as both poison and remedy, depending on the methodology that frames their use. Just as data-driven tools and methods can be used to invade personal privacy, fuel targeted marketing, and perpetuate large-scale public manipulation, they can also provide powerful mechanisms for observing and understanding digital data. In terms of digital visual studies, they can especially be powerful tools for studying the circulation and rhetorical consequentiality of digital visual artifacts such as Obama Hope.</p>
<p>Consider, for example, one of the most important projects in the areas of data accessibility and the preservation of digital data—The Internet Archive (archive.org), which works to preserve access to a broad diversity of digital sources with its WayBackMachine and other digital tools. As Blake Hallinan’s chapter in this collection makes transparent, the WayBackMachine provides a sustainable way to hyperlink webpages—ensuring that still-available webtexts do not suffer from <a href="https://en.wikipedia.org/wiki/Link_rot"><span class="underline">link rot</span></a>. For our research, we rely on a tool similar to The Internet Archive called <a href="https://www.docnow.io/"><span class="underline">DocNow</span></a>, which “support[s] the ethical collection, use, and preservation of social network data.” One of DocNow’s most promising features is its catalog of Twitter datasets, which has enabled DocNow to grow their archive of Twitter data—they now have a wide diversity of social movement and political datasets. For example, they archived 39,622,026 tweet IDs related to <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5QCCUU"><span class="underline">public discussions of climate change</span></a> from September of 2017 until May of 2019 (tags: climate change, environment, politics). They also have a <a href="https://archive.org/details/blacklivesmatter-tweets-2016.txt"><span class="underline">#blacklivesmatter dataset</span></a>, consisting of 17,292,130 tweets from January of 2016 until March of 2017 (tags: blacklivesmatter, activism). Taking advantage of this feature, the <a href="https://www.docnow.io/catalog/"><span class="underline">Obama Hope Tweets</span></a> dataset that we collected for this chapter is now available through DocNow, helping to ensure that this aspect of Obama Hope’s history is preserved and available for other researchers to use.</p>
<p>DocNow is just one example of how data driven methods and tools can be productive vehicles for driving digital visual studies research. In subsequent sections of this chapter, we thus explore how other data driven tools and methods can be useful for doing digital visual studies. More particularly, after describing what the humanities, broadly speaking, are calling <em>macroscopic</em> methodologies, we describe how deploying exploratory and descriptive methods (using <a href="http://www.massmine.org"><span class="underline">MassMine</span></a> and <a href="https://www.tidytextmining.com"><span class="underline">R</span></a>) help generate macroscopic visual histories of digital artifacts such as Obama Hope. As evidence, we present findings from research in which we collected and analyzed XXX14,000XXX Tweets that document conversations about Obama Hope between 2008 and 2016.</p>
<p>We ultimately argue that data driven methods easily extend the types of research we already do, by providing new insights into artifacts like Obama Hope, and eventually, showing promise to enable us to study the networks themselves—working to better understand the wide range of effects resulting from the transition from user-generated content to data-driven content in digital networks XXX explain first two sections here, and then transition XXX</p>
<p><strong>The Problems of Black-Boxed Underlying Digital Data</strong></p>
<p>Blackboxing, to be clear, occurs when technologies, and their underlying systems, are opaque (difficult to understand or make sense of), proprietary (closed to access for financial or legal reasons), closed off from critique and accountability in some other way. One problem with blackboxing is that without adequate access to underlying data, academic researchers have a difficult time studying and acknowledging the harmful contaminants emerging from the shift to data-driven content: bots, click farms, ideological echo chambers, information cascades, the unintended consequences of algorithmic filtering, the surveillance economy, privacy infringements, and fake news/misinformation campaigns. When it comes to studying the circulation and consequentiality of Obama Hope, for instance, bots and algorithms have likely played a major role in driving Obama Hope’s mobility and virality. As digital visual studies scholars, we argue that without understanding how such technologies are impacting such phenomena, we run the risk of misrepresenting how those phenomena come to be—misrepresentations that impact, in turn, the theories and knowledge we generate that potentially come to impact entire disciplines.</p>
<p>Another problem has to deal with the obsolescence of tools we use to study visual data due to the venture capital business model used to develop many technologies—obsolescence that not only impacts our ability to access researchers’ data visualizations but also our ability to replicate their data-driven research methods. In her data visualizations of Obama Hope, for instance, Gries used Google Fusion tables to store her data and run the geographical maps. Due to Google’s decision to no longer support this technology, Google Fusion Tables are now no longer available, and the data visualizations in Gries’ article are now broken (Gries). Gries did not realize the risk of these tools at the time she published the article, but now some of the data and results she shares in her article are incomplete and inaccessible for new readers. This is a direct result of the business model used by Google and others to develop and forward data-driven technologies. This “free” venture capital model—where users are given free access to tools and technologies, while companies build and test the tools as “proof of concept” for investors—has disastrous consequences for how we make and share knowledge in digital visual studies.</p>
<p>As another example, in 2015, Eunsong Kim published “The Politics of Trending” to demonstrate how Twitter’s trend monitoring algorithms were ignoring—and potentially suppressing—trends associated with #blacklivesmatter. Kim used Topsy—a “free” cloud tool for analyzing the data within various social networks—to conduct her research. Unfortunately, Topsy is now no longer usable. Apple purchased Topsy in 2013 for $200 million, and then proceeded to shut Topsy down entirely in December of 2015, making it no longer possible to replicate Kim’s methods for our own research. The former director of business development at Topsy, Aaron Hayes-Roth—who was unhappy with Apple’s decision to close Topsy—wrote an editorial trying to answer a simple question: why would Apple pay $200 million for a business like Topsy, only to close its doors two years later? According to Hayes-Roth, Apple wanted to use Topsy’s data mining technology and Topsy’s comprehensive Twitter dataset to compete with Google’s search/advertising services on Apple’s mobile devices. Thus, the erasure of Kim’s methods was a consequence of Apple trying to compete with Google. Unfortunately, this example is not an isolated incident—this is a direct consequence of a venture capital business model that drives the development of most digital technologies. The sarcastic profile description on Topsy’s still-available Twitter page summarizes the problem well: “Every Tweet ever published. <em>Previously</em> at your fingertips” (emphasis added).</p>
<p>Certainly, there are free cloud tools being developed precisely for academic research that provide an immense service for extending data science methods to digital visual studies, such as <a href="https://voyant-tools.org/"><span class="underline"> Voyant</span></a> (which is available as both a cloud tool and as standalone open source software). Voyant’s free cloud application provides natural language analytics that are easy to use, and we regularly use this tool in our classrooms and workshops to teach text mining and to introduce data-driven methods. Many other cloud tools like <a href="https://sumall.com/"><span class="underline"> SumAll</span></a> also appear useful for studying the circulation of digital visual artifacts. However, while useful, most of these tools were not built for peer-reviewed academic research—they were built explicitly to service a Big Data, venture capital business model. For example, SumAll’s justification for providing free access to data analytics is as follows: “Since we’re still working on building great products for small businesses, SumAll is free. Think of this as our soft launch—no credit cards and no subscriptions required.” SumAll is explicitly telling users their free access will not be sustained. Furthermore, we should look to the future and ask: how long until SumAll is acquired, fundamentally changed, or no longer available? Similarly, we might ask: if digital visual studies produces research using data from SumAll, and then SumAll is acquired and closed like Topsy, how can future scholars trust research that relies on data visuals that cannot be replicated, tested, or otherwise evaluated? Cloud applications like SumAll usually do not provide users with access to the underlying data and the methods used to produce their analytics—as these are considered “proprietary”—and, therefore, users of Big Data tools have to simply trust the visuals (analytics) presented to them. This may not be a problem for business data visualization practices, but it is a problem for academic research that relies so heavily on methodological replication and peer review. How, for instance, can digital visual studies scholars peer review data visuals that are based on proprietary, black boxed methods?</p>
<p>Finally, and this is something we have written about at length (Beveridge), the business model discussed throughout this section often limits access to proprietary digital data that we need to do digital visual studies at scale. In March of 2018, Facebook’s primary third-party data provider Datasift, for instance, was acquired by Meltwater, a business analytics firm specializing in data mining technologies. Similarly, Twitter’s primary data partner GNIP, which Twitter purchased in 2014, has now been fully merged with Twitter’s new “Enterprise” developer services. While Facebook continues to systematically reduce the free public access to their data, Twitter has showed renewed efforts to work closely with developers in addressing the many democratic challenges facing digital networks. This is a good sign for digital humanities researchers, as Twitter and GNIP have provided many useful datasets for answering challenging questions about the changing nature of social movements in digital environments and the effects of networking technologies on democratic discourse. However, the public availability of proprietary digital data (like Twitter data) for academic research has not significantly improved since 2015.</p>
<p>In our results section, we show how tools like MassMine and R help address these accessibility issues by providing sustainable open source tools for exploratory and descriptive social media research. But first, we describe the governing methodology of our research, which we identify as extending macroscopic methodologies that have been emerging throughout the humanities. Macroscopic methodologies, as we define them here, refer to theories about digital data, scale, information, and content that are constantly evolving due to the rapid rate of technological change. These methodologies are vital to inventing and adapting the methods of data science for digital visual studies. The humanities, broadly speaking, have utilized various neologisms in discussing and applying these methods—such as <em>distance reading</em> or <em>computational methods</em> or <em>procedural practices</em>—and outside the humanities other terminologies have been used to refer to data-driven methods, like <em>the</em> <em>fourth paradigm</em>, <em>informatics</em>, or <em>eScience</em>. The expanding utility for these methods in academic research is based on a simple yet powerful pragmatics: many of the systematic aspects of analysis or observation, currently undertaken by human reading or other forms of human observation, may be supplemented by computer programs that automate certain aspects of human reading/observation, making them reproducible for analyzing large collections of digital artifacts. In other words, the methods made available by data science and data mining toolsets continue to expand in value because they enable us to more accurately study digital artifacts <em>at scale</em>.</p>
<p>The following section delves into theories about digital data, information, content, and scale in order to explain why the ever-expanding size of digital data—the massive scales introduced by data-driven technologies—call for digital visual studies to adopt and revise data-science methods for our own research practices. Lev Manovich’s work with cultural analytics has made significant strides in observing, analyzing, and interpreting large data sets of visual images. As Manovich defines it, <em>cultural analytics</em> develops new data mining and data visualization research practices to analyze large collections of visual, multimodal, and other born-digital artifacts. Manovich’s <a href="http://lab.culturalanalytics.info/p/projects.html"><span class="underline">Cultural Analytics Lab</span></a> has studied the growth of image sharing on Twitter using 270 million geo-coded images, and his lab has conducted large-scale analyses of Instagram, asking: “What do millions of Instagram photographs tell us about the world?” The impressive study shows the immense potential of macroscopic methods for digital visual studies, but we argue that digital visual studies will benefit from expanding its repertoire of data driven methods to include exploratory and descriptive statistics that enable us to also understand the conversations, contexts, and histories of posts that impact the circulation of visual artifacts. As the next section will show, the exigence for data-driven methods is not only the benefit they provide in exploring and describing the data we collect, but also in dealing with the massive new scales introduced by data-driven technologies.</p>
<p><strong>Data, Information, Content and Scale</strong></p>
<p>In recent history, the scale of digital data has changed dramatically. Paradoxically so, the scale of digital content and its trace data has grown so large that it has toggled from being considered an ever-expanding problem--what has been colloquially called <em>information overload</em> or the <em>data deluge</em>-- to a new commodity and resource of value. Read almost any industry handbook on data science or decision making for business management, and you will likely find the following phrase: <em>data is the new oil</em>—a conception of data that inverts older <em>information overload</em> axioms such as “too many books too little time,” into a commodified, if not a dystopian motto.</p>
<p>In <em>Future Shock</em>—the book credited with popularizing “information overload” in the early 1970s— Alvin Toffler predicted the data-driven turn that has transformed data into a hot commodity. As Toffler explains, “Rational behavior, in particular, depends on a ceaseless flow of data from the environment. It depends upon the power of the individual to predict, with at least fair success, the outcome of his own actions.” Of course, Toffler admits this simple schema breaks down when “the individual is plunged into a fast and irregularly changing situation, or a novelty-loaded context.” In response to this problem, Toffler suggests that in order to compensate for higher speeds of change and greater amounts of information, a person "must scoop up and process far more information than before. […] In short, the more rapidly changing and novel the environment, the more information the individual needs to process in order to make effective, rational decisions” (180). In this conception, the information overload transforms information—or data—into a commodity. Rather than an overload or surplus functioning to devalue information and data as a whole (based on supply/demand approaches to valuation), it has instead, paradoxically, made data comparable in value to a natural resource we cannot live without.</p>
<p>Data has not just changed in quantity (and value) over the last few decades, of course. It has also changed in terms of how it is produced and stored, impacting what we consider to be, and ultimately how we study, digital content. It took approximately 14 years—from 1991 to 2005—for global Internet access to reach 1 billion people, and by 2005 many aspects of the Document Object Model for rendering web pages on computer screens (allowing for dynamic websites and interactive cloud applications using scripting languages such as JavaScript) began to receive full support from the top web browsers. By 2006, Myspace became the most visited website in the United States, YouTube was acquired by Google, Facebook transitioned to a public social network, Twitter was officially launched, Reddit opened its news posts to interactive commenting, and Wikipedia reached 1 million articles and began shifting its focus to improving the quality of its crowd-sourced content. While the influence of Napster and peer-to-peer file sharing on 2005’s emerging “Web 2.0” environment should not be downplayed, one phrase seems more indicative of this new era of digital technology than all the others: the era of <em>user-generated content</em>.</p>
<p>With the advance of digital photography and mobile devices, the era of user-generated content gave way to an immense expansion of digital data. Even though Apple was still a year away from releasing the first iPhone, and smartphone sales would not outpace standard mobile phone sales for another 7 years, in 2006 digital cameras captured approximately 150 billion images and mobile phones snapped an additional 100 billion as well. The transition to digital cameras as a data format for images (as opposed to the printing of photographs) was so disruptive that Kodak, Sony, Fuji, and Nikon were all forced to undertake fundamental changes in how they provided technologies and services for image capture. Furthermore, similar changes also occurred for video recording and distribution. By 2006 YouTube was already hosting nearly 100 million user-generated video streams per day, and in that year more than 1 billion music MP3s were shared over the Internet each day. In 2006 alone, 161 exabytes (161 billion gigabytes) of digital content and data were generated—equivalent to 3 million times the total approximate data contained in <em>every book ever written</em> (“Expanding”). Two decades earlier, in 1986, the total amount of global digital content and data in storage was 2.6 exabytes, and by the end of 2006 this grew to 295 exabytes in total storage—equal to 61 CD-ROM discs of data per-person globally. By 2010, the total amount of global digital storage amassed 1227 exabytes of data, outpacing previous estimates by 239 exabytes, and by 2020 it is estimated that global digital storage will reach 40,000 exabytes of data—equivalent to 5.2 terabytes for every human on the planet (“Digital”).</p>
<p>Of course, the total amount of global data generated is not simply a product of the data attributed to <em>digital content or digital artifacts</em>. As the industry study cited above notes, “the amount of information individuals create themselves—writing documents, taking pictures, downloading music, etc.—is far less than the amount of information being created <em>about</em> them in the digital universe” (“Digital,” emphasis added). Every query conducted within a search engine, every liked post on a social network, every visited website and clicked hyperlink, every digital purchase and online transaction, and every written document, shared image, and streamed video creates data <em>about</em> user behavior and activity—and about the artifacts themselves. Thus, as we head into the 6th decade of public networked computing—from the walled gardens of the intranets to the early years of the Internet, and then from the early Internet to the era of user-generated content—we are witnessing a new era of networked computing and digital content: the era of <em>data-driven content</em>. Browsing, surfing, and searching have been replaced with algorithms, filters, and feeds. Rather than humans searching the Internet or scanning social networks to discover digital visual content, new content is delivered to users directly through programs that "learn" their favorite types of content or through filters that select content based on the topics and categories users “engage” most frequently. And these programs respond to the data collected from user profiles, search histories, purchase patterns, viewing habits, likes, comments, interactive behaviors, and from a multitude of sensor data provided by mobile phones and other internet-connected devices—producing <em>even more</em> data.</p>
<p>Such massive amounts of data demand that digital visual studies scholars develop research methods that keep pace with the scales of content being produced everyday by citizens and businesses alike. The term scale seeks balance—as in the scales of justice or a scale that weighs an object. For example, computer graphics are <em>scalable</em> when they retain their initial clarity as digitally rendered images expand or change in size significantly on a screen. In art, when a sketch or model is <em>drawn to scale</em>, its proportionality stays in tact when the artist paints their sketch as a large mural or carves them into an immense statue from stone. Broadly speaking, the <em>scalability</em> of conventional systems and methods function well within normal deviations in size, but with massive—exponential—increases in scale, the same systems and methods will fail to remain <em>proportionally</em> <em>effective</em>. For example, common techniques for constructing a twenty-floor office building will not erect a mile-high skyscraper. Propulsion systems that once pushed a spacecraft to the moon are incapable of transporting humans to the next closest solar system. And of course, email, the long since standard of digital communication, now seems as impotent as postal mail in a world with over 4.3 billion internet users. The lesson of scale is this: all systems and methods will likely fail to remain proportionally effective at dramatically different scales. Put simply, no system or method will scale infinitely. Therefore, if we want to develop effective descriptions of visual content, as researchers, we must continue to invest in methods that allow us to keep pace with the ever-increasing scale of data <em>about</em> visual content.</p>
<p>In both history and literature, substantial progress has been made to begin applying historical and literary theories to the new scales made available through resources such as the Google Books corpus, Gutenberg, and Hathi Trust. Substantial progress has also been made in applying data-driven tools and methods to the large-scale analysis of historical documents and texts. Matthew Jockers’ <em>Macroanalysis: Digital Methods and Literary History</em>, for instance, provides an extensive rationale for data-driven research in literary studies, and Jockers has gone so far as to produce a companion textbook to supplement this work: <em>Text Analysis in R for Students of Literature</em>. Likewise in <em>Exploring Big Historical Data: The Historian’s Macroscope</em>, Shaun Graham, Ian Milligan, and Scott Weingart provide a comprehensive overview of data-driven methods for historians, building on other valuable resources in this area, like the archive of tutorials available at <a href="https://programminghistorian.org/"><em><span class="underline">The Programming Historian</span></em></a>. Digital visual studies scholars have much to learn from such work in thinking about how to implement macroscopic methods for our own research. Gries’ work with circulation and “tracking” Obama Hope, in particular, acts as a precursor and a methodological bridge for seriously considering how to apply data-driven methods to pressing questions in digital visual studies. While Gries’s work introduces uses of data-driven methods to account for the rhetorical life of Obama Hope, Gries has admittedly only gestured to the possibilities that can be harnessed by these tools.</p>
<p>If digital visual studies is to flourish, we need to keep pushing to improve our ability to study digital networks, the artifacts that circulate within them, and the massive amounts of data they produce. Given the problems with obsolescence, replication, peer review, and access discussed in the previous section, this will mean continuing to understand and study both the business models and development practices that invent new digital technologies to know when and how the networks change—changes that directly affect <em>how</em> we study them. Data driven methods have developed alongside digital technologies, precisely to enable such studies: at their most fundamental level they are designed to provide feedback on digital activities of all kinds, including substantive changes to networks and their underlying systems and programs that impact our access to data and, ultimately, the stories and knowledge we are able to generate about digital visual artifacts.</p>
<p>Admittedly, studying the networks themselves remains a larger, long term goal for macroscopic methodologies. Yet, we can begin working toward that goal by improving our fluency with data-driven tools in general, applying them to the types of research we already conduct, with an eye toward inventing research practices that account for the artifacts and phenomena that we are already studying—but at much larger scales. Macroscopic methods (here defined as exploratory and descriptive statistics) enable such research and push us to ask: what aspects of our massive image collections can be automated? When researching visual artifacts such as Obama Hope, how can we use already-available metadata, captions, and comments to aid our coding, interpretations, and analyses? And, finally, how can we make these practices reproducible, so that other researchers can make use of the programming scripts, data processing techniques, and visualization practices in when testing our results (peer review) or when trying to think of new ways to analyze or compare visual data?</p>
<p>In the following section, we demonstrate how exploratory and descriptive methods can help to make sense of the reactions, responses, and comments about visual artifacts on social media. We particularly wanted to know what topics related to Obama Hope emerged on Twitter between 2008 and 2016, the years of Obama’s administration. And consulting with Gries, we explore what the data has to say about Obama Hope, and how this offers useful insights into the artist behind Obama Hope, Sheppard Fairey. We begin by providing a definition of exploratory and descriptive methods, and then we identify the methods we used to produce the results/visuals, and finally present and comment on our findings.</p>
<p><strong>Methods and Findings</strong></p>
<p>Exploratory and descriptive statistics are just that—exploratory and descriptive. When we explore data, our goal is not to confirm or falsify, but to simply look for interesting patterns or data that raise questions or inspire further inquiry. When we describe data, we are not strictly predicting or testing, but merely doing our best to provide a careful portrayal of some aspect of our data: changes over time (time series), categorical descriptions (genres, topics, terms), quantitative totals (sums, averages, percentages, frequencies, etc.), maps/locations (chloropleths, coordinates), and many others. It would be too strong a claim to suggest that our descriptions and explorations in this section are representative of Twitter users’ opinions about Obama Hope, and it would also be a mistake to suggest our findings represent public opinion more generally. Our goal is to explore the reactions, comments, and other responses to Obama Hope—to get a sense of how those reactions, comments, and responses may have changed over Obama’s administration.</p>
<p>When we tested possible search queries through <a href="https://twitter.com/search-advanced"><span class="underline">Twitter’s advanced search</span></a> functionality, the most obvious search terms like “Obama Hope” returned lots of data, but they also had substantial numbers of tweets that had nothing to do with Fairey’s design. For example, many tweets said things like: “We sure hope Obama can fix healthcare” and other commentary not referring to Obama Hope. We thus tested many other phrases and word combinations until we settled on the following: (1) “Fairey AND Hope,” and (2) “Obama AND Hope AND poster”—two separate queries. These two queries allowed us to harvest IDs for the 25,XXX tweets in our dataset, which we then uploaded to the DocNow! archive. This edited collection includes the “Hope Archive: Datasets and Artifacts for Future Research,” and this archival section of <em>Doing Digital Visual Studies</em> provides full access to the tweets used in this chapter, the full Obamicon dataset, and the new Trumpicon dataset. The “Hope Archive” also includes instructions for accessing the data, and resources for beginning to explore and describe those datasets for readers’ own research projects.</p>
<p>In the descriptions that follow, we show, for each year in our dataset, (1) the most favorited tweet, (2) the most retweeted tweet, (3) and the top 10 most frequently occurring words in the tweets themselves. For 1 and 2, we simply used the metadata from Twitter—returned by <a href="http://www.massmine.org"><span class="underline">MassMine</span></a>—when we rehydrated the tweet IDs (Van Horn and Beveridge). For every tweet in the dataset, Twitter provides numeric totals on the number of favorites and retweets each tweet received, and we have selected and displayed below the tweets that have received the highest number of favorites and the highest number of retweets for each year. When the most favorited and most retweeted tweet, for each year, were the same tweet, we selected the second most retweeted tweet to remove redundancies.</p>
<p>For determining word frequencies, we used the <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"><span class="underline">tf-idf</span></a> method. Since tweets are tiny documents, and tf-idf favors terms that appear frequently in one document, but less (or, ideally, never) in other documents, we ran into the problem with odd phrases producing nonsensical terms with very high tf-idf scores. Having a list of top-10 terms that don’t yield meaningful insights effectively renders the method useless. To combat this problem we treated each year of tweets as a separate corpus. Within each corpus, we concatenated all tweets into a single string of text (i.e., a single "document"). This gave us 108 documents, or 12 documents per year (technically, there were only 107 documents, as one month had zero tweets and so doesn't appear in the dataset<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>), thus producing a document for each month of tweets. The result is a single figure for each year that shows the top-10 terms for each year. The text in the documents was reduced to the most meaningful words by removing filenames, stop words (words like: the, and, it, at), by removing extra white space (so that each word is separated by one space in each document), and by transforming hyphenated words into two separate words. These are standard “data janitorial” techniques (Beveridge). For further information on these and other similar methods, see <a href="https://www.tidytextmining.com"><em><span class="underline">Text Mining with R</span></em></a>. This book provides a great introduction to text mining, in particular, and it contains many code examples—including code examples that reproduce the word frequency graphs used in this section.</p>
<p>This approach to “reading” the text contained within a year’s worth of tweets truly provides a meaningful definition of the term <em>macroscopic</em>, as things like grammar, syntax, and punctuation are all meaningless to this type of <a href="https://en.wikipedia.org/wiki/Bag-of-words_model"><span class="underline">bag-of-words</span></a> reading. In fact, the computer does not “read” at all. It treats unique strings of characters (words) as things to count and categorize. So when certain words show up in the top-10 lists of our word frequency graphs, it’s not because the computer knows the definitions of those words when choosing them for the graph. Rather, they show up in the graphs only because our method of counting determined that these were the most frequent strings of characters (words). The information in this graph only becomes meaningful when contextualized and interpreted by a human reader.</p>
<p>Additionally, we did preserve the @ and # characters in our dataset, as standard punctuation removal techniques usually scrub these characters from text data. However, no @usernames or #hashtags occurred frequently enough in any given year to be displayed as top-10 terms in our figures. We tested raw word-counts for yearly word frequencies as well, to make sure that this omission was not an unintended consequence of the tf-idf method. Often, when we explore the most frequent terms from a given Twitter dataset, #hashtags and @usernames will appear occasionally among the most frequent terms, but for this dataset #hashtags and @username mentions occurred less frequently than the other words that remained after scrubbing the data.</p>
<p>Below you will find three figures of Twitter data…XXX basic description of the actual figures that follow to conclude XXX</p>
<p><strong>Yearly Descriptions</strong></p>
<p>[results go here]</p>
<p><strong>Insights Yielded</strong></p>
<p>In many ways, the Twitter data above demonstrates that while Obama Hope clearly drew the attention of Twitter followers, it was Shepard Fairey himself who catalyzed many reactions, comments, and responses. In <em>Still Life with Rhetoric</em>, Gries makes Obama Hope the central character in her four part case study, revealing the ways that Obama Hope, as an important actor in its own right, catalyzed a plethora of rhetorical consequences around the world. While surely Obama Hope did attract global attention and inspire thousands of remixes that became part of important collective activities around the world, the Twitter data above suggests that as much as Obama Hope gained fame from its circulation, so too did Shepard Fairey. Whether retweeting links to his appearance on the Colbert Report or liking the fact that someone met him in person or liking tweets about his new designs, Twitter followers were captivated with Shepard Fairey.Before producing and distributing Obama Hope, Fairey was already famous for his Obey campaign, as evident in the most favorited tweet in 2009 in which he was referred to as “Shepard Fairey of Obey Giant.” But this Twitter data suggests, that once Obama Hope began to circulate and gain mass attention, Fairey began to attract widespread social media attention for a variety of actions. The word frequency graphs, read chronologically, actually reveal how such attention shifted with new developments in Fairey’s (and Obama Hope’s) life. In 2008, for instance, we see Fairey gaining attention for his Obama Hope design, which at that point was already being called iconic. By 2009, the copyright scandal with AP over Fairey’s Obama Hope design entered into the spotlight, a scandal that occupied Twitter attention through 2012. In 2010, for instance, the most favorite tweet pertained to Fairey’s “‘Hope’ Trial” and his “novel defense” while in 2012, the most retweeted tweet pertained to Fairey’s 2 year probation sentence. As street artists, people are naturally interested in the way Fairey becomes involved in, as one tweet put it, “Art and Crime,” so it will come as no surprise that it was not only his AP trouble that gained attention but also his 2015 arrest in Detroit. Interestingly, by that time, Fairey once mainly associated with Obey Giant was now commonly referred to as “the guy” or artist behind the Obama Hope poster.</p>
<p>Fairey, of course, captured Twitter attention for other reasons as well. To no surprise, perhaps, when he released new designs, such as his “Occupy Hope” design with a Guy Fawkes mask replacing Obama in the portrait, he was tweeted about. For instance, in 2016, the most favorited tweet was Michael Moore’s thank you tweet to Fairey for designing a poster promoting Moore’s documentary <em>Where to Invade Next.</em> But more surprising, the Twitter data above discloses how many Twitter followers became interested in Fairey’s own political ideas. In one sense, as evident in the most retweeted tweet from 2010, people were interested in Fairey’s ideas about Obama. As an artist who is often credited for doing much to win Obama the oval office, it was intriguing to many, obviously, that Fairey began to speak out regarding his disappointment that his hope in Obama was “not panning out.” In 2016, for instance, the most retweeted tweet was about Fairey calling Obama a “failure”—a word that made it to the top-ten term list that year according to the word frequency graph from that year. But the Twitter data also suggests that people were interested in Fairey’s opinions about political art as a means of propaganda. In 2013, for instance, the most retweeted tweet quoted Fairey saying “You have to accept that visuals are a form of propaganda.” Such revelation, of course, was not new to many. Four years earlier, in 2009, the most retweeted tweet was “New rules for 2010: 1.) We’re outlawing rendering anything like a Ché Guevara poster or Obama ‘Hope’ poster.” As Gries’ rhetorical biography of Obama Hope also reveals, many resented Obama Hope and Fairey for infusing American politics with such popular and powerful political propaganda. It is no wonder then that even as Fairey supported Bernie in 2016, he decided not to generate a poster for that election, choosing instead to focus his attention on other political matters such as the protest of Trump, for which he designed the now iconic “We the People” series.</p>
<p>While such findings may not be revelatory in terms of Fairey’s own biography, this Twitter data does point to an unrecognized inquiry about Obama Hope that Gries’ and other’s work with Obama Hope has yet to yield—a question that we think is important for scholars interested in visual studies to more deeply consider. What is the complex impact that a single piece of art has on the artist him/her/theirself? While Gries did discuss in <em>Still Life with Rhetoric</em>, of course, the ways in which Fairey was caught up in the copyright scandal as well as ensuing accusations of propaganda, this Twitter data discloses that Obama Hope’s impact on Fairey himself is likely to more complex than we have yet to discover, especially as Obama Hope’s consequentiality on Fairey’s life only continues to unfold. What more about Obama Hope, then, might we learn if we made concerted efforts to study how it has impacted Fairey’s own life in both positive and negative ways?</p>
<p>In addition, the Twitter data above indicates that concentrated social media studies of visual artifacts have potential to identify previously undiscussed rhetorical consequences that those artifacts have helped to catalyze. For instance, while we did not mention this above, in 2014, the most retweeted tweet shared a link to a <a href="https://slate.com/culture/2014/09/gotham-typeface-tobias-frere-jones-font-from-obama-hope-poster-defines-our-era.html"><span class="underline">Slate article</span></a> about how the Gothom font used in the Fairey’s Obama Hope design has come to define our contemporary era. In the article, Obama Hope functions as a representative anecdote to illustrate just how ubiquitous the Gothom font has been. In a related article on the same topic, the author goes so far to post a picture of Obama Hope and claim that “One of the most iconic and widely received messages ever laid out in Gotham consisted of a single word: “HOPE” (Hawley). The author also goes on to discuss how the Obama Hope design has made a “lasting impression” on American politics, quoting one graphic designer who claims “I think the whole Obama design program changed the look of politics” (qtd. in Hawley). While Gries has acknowledged the importance of Obama Hope in American politics in other ways, Gries missed this important rhetorical contribution. When we adopt exploratory and descriptive methods in our digital visual studies research then, we create opportunities for not only new research questions and paths to open up but also new evidence to emerge for our own rhetorical research agendas.</p>
<p><strong>Conclusion: Future Explorations</strong></p>
<p>To end, we want to return to a point we made earlier about how data science methods not only lend insights to the visual artifacts themselves, but also how they help us to better understand the networks in which the artifacts circulate. While it may take some time—and serious methodological work—there are very tangible ways we can begin contributing to the work of studying the networks themselves. We can start by paying better attention to how data is used in digital networks, how the data affects algorithms and filtering programs, and how all of this may sometimes work in unintended ways—allowing bots, click farms, and other manipulative efforts to sway massive networks, with millions and billions of users. This is no small problem, but small efforts could make a big difference in combating and reducing some of the manipulative forces. Instagram is already testing the effect of removing <em>likes</em><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> from their platform, and Instagram believes this could result in people focusing more so on genuine interactions with users/content—rather than focusing too much on the number of likes an image receives.</p>
<p>Just as some data sources are paired down—as with the <em>like</em> button—others are continuing to expand in their influence and utility for digital networks. Emoji are one such data source, and they are quickly displacing the affective binary of like/dislike. By allowing for a much broader range of affective visual response, emoji replicate the efficiency of the one-click like/dislike, but they also enable a much broader range of responses—allowing for far greater affective resolution. While a sizeable portion of digital data remains either the alphabetic text produced by humans or the metadata attached to (or created by) digital artifacts, emoji are quickly becoming the standard for analyzing the attitudes and emotions that contribute to the reception and circulation of digital media.</p>
<p>Kellie Gray’s research makes a significant contribution to the study of emoji in digital networks—and to the study of the networks themselves—as she identifies key problems facing scholars who are interested in understanding the rhetorical impact of emoji. In addition to her full length dissertation on the subject—as well as her forthcoming article examining the function of emoji “as a form of multimodal public rhetoric”—Gray’s work shows how many of the programs designed to collect data from social media either fail to include emoji in-line with alphabetic script as originally composed, fail to include modifiers applied to specific emoji (such as skin tone or gender), or fail to include the emoji altogether. While these problems persist, emoji are becoming increasingly relevant as they facilitate new types of hybrid global communication. It is difficult to deny the emerging cultural and linguistic significance of emojis, but there are currently no research tools available that facilitate the broad scale research of emoji use within/across multiple digital networks. Thanks to Gray’s work, the NEH has funded the <a href="http://www.massmine.org/about.html">MassMine team</a> to develop workable solutions to these (and other) problems for researchers, and Gray is a funded contributor and co-Pi on the project.</p>
<p>We want to finish this conclusion with another observation of our own—an observation that, like Gray’s discovery about the problems with emoji data—we also discovered when collecting data about digital artifacts. As MassMine’s original data source, Twitter continues to provide a reasonable amount of access to free data for academic research, and they have shown a willingness to be more transparent about how they moderate the content in their network. Yet, there is a limit to the amount of transparency Twitter provides.</p>
<p>This limited transparency brings my back to the example of Eunsong Kim’s “The Politics of Trending,” and the fact that we are no longer able to replicate Kim’s methods. In addition to Apply purchasing and closing down Topsy, there is second reason why it is no longer possible to replicate the results of Kim’s research: Twitter no longer allows it. Twitter’s newest terms of service—which were likely revised in response to tools like Topsy—reads as shown in Figure 1:</p>
<p><img src="media/image1.png" style="width:6.5in;height:2.01875in" /></p>
<p>Figure 1 is a screencapture of <a href="https://web.archive.org/web/20190727194551/https:/developer.twitter.com/en/developer-terms/agreement-and-policy.html">Twitter’s terms of service from July 2019</a>, and it shows how Twitter now explicitly blocks the use of data from their API for benchmarking or testing the effectiveness (“responsiveness”) of Twitter as a network. In other words, Twitter blocks the use of their proprietary data—data that they received for free from users who post on their network—and if those same users want to aggregate that data to test the effectiveness of Twitter, that is explicitly prohibited. Twitter’s developer rules stops users from aggregating their data and testing whether or not Twitter—or Twitter’s advertisers—are overtly manipulating the content circulating in trends and user feeds.</p>
<p>Kim’s “The Politics of Trending” stands as an important example for reconsidering the long-term sustainability of our research practices more generally, but more specifically, it encourages us (1) to develop our own tools and data-driven technologies, (2) to learn how to apply a wider range of exploratory and descriptive statistical methods, and eventually, (3) to learn how to study the networks themselves—<em>at scale</em>. It’s not at all clear whether learning to apply the exploratory and descriptive methods of data science to the study of digital artifacts will lead in the end to such work contributing to improvements in the actual networks. But we should try, because as the underlying technologies of digital networks shift and change, so too do the possibilities for effective digital persuasion shift and change as well. If we are to have any agency over the future of digital networks, an important first step is working to better understand the business models that build and sustain digital technologies, as well as coming to terms with the tools, methods, and methodologies that enable us to study the networks themselves, and the digital (visual) artifacts that circulate within them.</p>

<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Technically, there were only 107 documents, as one month had zero tweets and so it doesn't appear in the dataset<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn2" role="doc-endnote"><p><a href="http://aaronbeveridge.com/book-site/chapters/hallinan/introduction.html"><span class="underline">Blake Hallinan</span></a> has produced an entire dissertation, dedicated to the <em>like</em> button.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>
